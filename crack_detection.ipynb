{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c28ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os, json, glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286d7a0",
   "metadata": {},
   "source": [
    "### **Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2988fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to remove unwanted labels from annotation JSON files.\n",
    "DROP_LABELS = {'Bearing', 'EJoint', 'Drainage', 'PEquipment', 'JTape', 'WConccor'}\n",
    "\n",
    "def clean_annotation(in_json_path: str, out_json_path: str, drop=DROP_LABELS) -> int:\n",
    "    \"\"\"Removes shapes whose 'label' is in DROP_LABELS. Returns # removed.\"\"\"\n",
    "    with open(in_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ann = json.load(f)\n",
    "\n",
    "    shapes = ann.get(\"shapes\", [])\n",
    "    kept = [s for s in shapes if s.get(\"label\") not in drop]\n",
    "    removed = len(shapes) - len(kept)\n",
    "    ann[\"shapes\"] = kept\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_json_path), exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(ann, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return removed\n",
    "\n",
    "def batch_clean_annotation(in_ann_dir: str, out_ann_dir: str, pattern=\"*.json\") -> None:\n",
    "    jsons = glob.glob(os.path.join(in_ann_dir, pattern))\n",
    "    total_removed = 0\n",
    "    for jp in tqdm(jsons, desc=\"Cleaning annotations\", total=len(jsons)):\n",
    "        rel = os.path.relpath(jp, in_ann_dir)\n",
    "        outp = os.path.join(out_ann_dir, rel)\n",
    "        total_removed += clean_annotation(jp, outp)\n",
    "    print(f\"Done. Processed {len(jsons)} files. Removed {total_removed} shapes total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8189ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning annotations: 100%|██████████| 6935/6935 [00:02<00:00, 2351.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Processed 6935 files. Removed 5859 shapes total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning annotations: 100%|██████████| 975/975 [00:00<00:00, 2085.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Processed 975 files. Removed 885 shapes total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean dataset annotations\n",
    "pattern = \"*.json\"\n",
    "\n",
    "# Train\n",
    "in_ann_dir = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data/annotations/train\"\n",
    "out_ann_dir = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/annotations/train\"\n",
    "batch_clean_annotation(in_ann_dir, out_ann_dir, pattern)\n",
    "\n",
    "# Validation\n",
    "in_ann_dir = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data/annotations/test\"\n",
    "out_ann_dir = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/annotations/test\"\n",
    "batch_clean_annotation(in_ann_dir, out_ann_dir, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e90b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing overlaps: 100%|██████████| 6935/6935 [04:23<00:00, 26.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed images: 6935\n",
      "Images with overlaps: 4042 (58.3%)\n",
      "Total overlapping pixels: 915524988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to draw annotations and compute overlaps\n",
    "def compute_overlap(img_path, ann_path, classes):\n",
    "    w, h = Image.open(img_path).size\n",
    "    # one boolean mask per class\n",
    "    planes = np.zeros((len(classes), h, w), dtype=bool)\n",
    "\n",
    "    with open(ann_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ann = json.load(f)\n",
    "\n",
    "    # Loop over shapes (one shape is one mask)\n",
    "    for shp in ann.get(\"shapes\", []):\n",
    "        label = shp.get(\"label\", \"\")\n",
    "\n",
    "        # Skip unwanted labels (should not be present after cleaning)\n",
    "        if label not in classes:\n",
    "            continue\n",
    "\n",
    "        # Get points for each shape (mask)\n",
    "        idx = classes.index(label)\n",
    "        pts = shp.get(\"points\", [])\n",
    "\n",
    "        # Skip invalid polygons if they exist\n",
    "        if len(pts) < 3:\n",
    "            continue\n",
    "\n",
    "        # Generate mask for this polygon, draw it to a temporary image, and add it to the corresponding plane (accumulating all masks for that class)\n",
    "        poly = [(float(x), float(y)) for x, y in pts]\n",
    "        m = Image.new(\"1\", (w, h), 0)\n",
    "        ImageDraw.Draw(m).polygon(poly, outline=1, fill=1)\n",
    "        planes[idx] |= np.array(m, dtype=bool)\n",
    "\n",
    "    # pixels covered by >= 2 classes\n",
    "    overlap = planes.sum(axis=0) >= 2\n",
    "    return planes, overlap\n",
    "\n",
    "# Relevant directories and classes\n",
    "IMAGES_DIR = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data/images/train\"\n",
    "ANNS_DIR   = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/annotations/train\"\n",
    "DEFECTS = ['Crack', 'ACrack', 'Wetspot', 'Efflorescence', 'Rust', 'Rockpocket', 'Hollowareas', 'Cavity', 'Spalling', 'Graffiti', 'Weathering', 'Restformwork', 'ExposedRebars']\n",
    "\n",
    "# Get annotations\n",
    "jsons = sorted(glob.glob(os.path.join(ANNS_DIR, \"*.json\")))\n",
    "n_imgs = 0\n",
    "n_overlap_imgs = 0\n",
    "overlap_pixels_total = 0\n",
    "\n",
    "# Compute overlaps\n",
    "for jp in tqdm(jsons, desc=\"Computing overlaps\", total=len(jsons)):\n",
    "    stem = os.path.splitext(os.path.basename(jp))[0]\n",
    "    ip = os.path.join(IMAGES_DIR, f\"{stem}.jpg\")\n",
    "    if not os.path.exists(ip):\n",
    "        continue\n",
    "    n_imgs += 1\n",
    "    _, ov = compute_overlap(ip, jp, DEFECTS)\n",
    "    n = int(ov.sum()) # number of overlapping pixels\n",
    "    if n > 0:\n",
    "        n_overlap_imgs += 1\n",
    "        overlap_pixels_total += n\n",
    "\n",
    "print(f\"Processed images: {n_imgs}\")\n",
    "print(f\"Images with overlaps: {n_overlap_imgs} ({100.0*n_overlap_imgs/max(1,n_imgs):.1f}%)\")\n",
    "print(f\"Total overlapping pixels: {overlap_pixels_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8d75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6935/6935 [01:01<00:00, 113.57it/s]\n",
      "100%|██████████| 6935/6935 [00:00<00:00, 6706824.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pixels in dataset: 29073367793\n",
      "Ratio of overlapping pixels: 0.0315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Count total number of pixels to compute ratio\n",
    "path = \"data/images/train\"\n",
    "img_paths = [os.path.join(folder, file) for folder, _, files in os.walk(path) for file in files]\n",
    "img_shapes = [cv2.imread(img_path).shape for img_path in tqdm(img_paths)]\n",
    "img_pixels = [shape[0]*shape[1] for shape in tqdm(img_shapes)]\n",
    "\n",
    "print (f\"Total pixels in dataset: {sum(img_pixels)}\")\n",
    "print (f\"Ratio of overlapping pixels: {overlap_pixels_total / sum(img_pixels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5239c5a",
   "metadata": {},
   "source": [
    "Because the dataset annotations are multilabel and overlap in ~3% of pixels, we collapse overlaps to a single class using a fixed priority hierarchy. This enables consistent multiclass training and evaluation for U-Net, SegFormer, and Mask2Former. A multilabel experiment will be explored separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "49c749ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest priority first (wins on overlap)\n",
    "PRIORITY = [\n",
    "    'Crack', 'ExposedRebars', 'Spalling', 'Rust', 'ACrack',\n",
    "    'Rockpocket', 'Hollowareas', 'Efflorescence', 'Cavity',\n",
    "    'Wetspot', 'Weathering', 'Restformwork', 'Graffiti',\n",
    "]\n",
    "\n",
    "PRIORITY_DICT = {cls: i+1 for i, cls in enumerate(PRIORITY)}  # 0 = background\n",
    "\n",
    "def rasterize_polygon_mask(w, h, pts):\n",
    "    m = Image.new('1', (w, h), 0)                   # 1-bit mask\n",
    "    ImageDraw.Draw(m).polygon([(float(x), float(y)) for x,y in pts],\n",
    "                              outline=1, fill=1)\n",
    "    return np.array(m, dtype=bool)\n",
    "\n",
    "def labelme_to_multiclass_png(json_path, out_png_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        ann = json.load(f)\n",
    "\n",
    "    # image size\n",
    "    w, h = int(ann[\"imageWidth\"]), int(ann[\"imageHeight\"])\n",
    "\n",
    "    # per-class boolean planes (K,H,W)\n",
    "    planes = {cls: np.zeros((h, w), dtype=bool) for cls in PRIORITY}\n",
    "\n",
    "    # accumulate polygons into planes\n",
    "    for shp in ann.get(\"shapes\", []):\n",
    "        label = shp.get(\"label\", \"\")\n",
    "        pts = shp.get(\"points\", [])\n",
    "        if label in planes and len(pts) >= 3:\n",
    "            planes[label] |= rasterize_polygon_mask(w, h, pts)\n",
    "\n",
    "    # collapse to single-channel mask (uint8), background = 0\n",
    "    Y = np.zeros((h, w), dtype=np.uint8)\n",
    "    # iterate from LOWEST → HIGHEST so highest priority overwrites last\n",
    "    for cls in reversed(PRIORITY):\n",
    "        Y[planes[cls]] = PRIORITY_DICT[cls]\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_png_path), exist_ok=True)\n",
    "    Image.fromarray(Y, mode='L').save(out_png_path)   # grayscale PNG\n",
    "\n",
    "def batch_convert_labelme_to_png(ann_dir, out_dir, pattern=\"*.json\"):\n",
    "    paths = sorted(glob.glob(os.path.join(ann_dir, pattern)))\n",
    "    for jp in tqdm(paths, desc=\"Converting to PNG\", total=len(paths)):\n",
    "        stem = os.path.splitext(os.path.basename(jp))[0]\n",
    "        outp = os.path.join(out_dir, f\"{stem}.png\")\n",
    "        labelme_to_multiclass_png(jp, outp)\n",
    "    print(f\"Done. Wrote {len(paths)} masks to: {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d698e1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to PNG: 100%|██████████| 6935/6935 [02:17<00:00, 50.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote 6935 masks to: /Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/masks/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning annotations: 100%|██████████| 975/975 [00:00<00:00, 2939.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Processed 975 files. Removed 0 shapes total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "in_ann_dir = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/annotations/train\"\n",
    "out_mask_dir = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/masks/train\"\n",
    "batch_convert_labelme_to_png(in_ann_dir, out_mask_dir)\n",
    "\n",
    "# Test\n",
    "in_ann_dir = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/annotations/test\"\n",
    "out_ann_dir = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/masks/test\"\n",
    "batch_clean_annotation(in_ann_dir, out_ann_dir, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef913ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each label\n",
    "LABELS = [\n",
    "    'Crack', 'ExposedRebars', 'Spalling', 'Rust', 'ACrack',\n",
    "    'Rockpocket', 'Hollowareas', 'Efflorescence', 'Cavity',\n",
    "    'Wetspot', 'Weathering', 'Restformwork', 'Graffiti',\n",
    "]\n",
    "\n",
    "# Function to count labels in annotations\n",
    "def count_labels(ann_paths, LABELS):\n",
    "\n",
    "    # Initialize counts\n",
    "    label_counts = {label: 0 for label in LABELS}\n",
    "\n",
    "    # Loop over annotations and count labels\n",
    "    for ann_path in tqdm(ann_paths, desc=\"Counting labels\", total=len(ann_paths)):\n",
    "        with open(ann_path, 'r', encoding='utf-8') as f:\n",
    "            ann = json.load(f)\n",
    "        labels_in_img = set()\n",
    "        for shp in ann.get(\"shapes\", []):\n",
    "            label = shp.get(\"label\", \"\")\n",
    "            if label in LABELS:\n",
    "                labels_in_img.add(label)\n",
    "        for label in labels_in_img:\n",
    "            label_counts[label] += 1\n",
    "    \n",
    "    return label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a1cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting labels: 100%|██████████| 6935/6935 [00:00<00:00, 11014.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts in training set:\n",
      "  Crack: 1727\n",
      "  ExposedRebars: 773\n",
      "  Spalling: 3262\n",
      "  Rust: 3450\n",
      "  ACrack: 332\n",
      "  Rockpocket: 311\n",
      "  Hollowareas: 1097\n",
      "  Efflorescence: 1523\n",
      "  Cavity: 1188\n",
      "  Wetspot: 965\n",
      "  Weathering: 2704\n",
      "  Restformwork: 713\n",
      "  Graffiti: 795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "ann_dir_train = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/annotations/train\"\n",
    "\n",
    "# Get all annotation paths\n",
    "ann_paths_train = [os.path.join(ann_dir_train, sp) for sp in os.listdir(ann_dir_train)]\n",
    "label_counts_train = count_labels(ann_paths_train, LABELS)\n",
    "print(\"Label counts in training set:\")\n",
    "for label, count in label_counts_train.items():\n",
    "    print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c130cb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting labels: 100%|██████████| 975/975 [00:00<00:00, 8656.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts in validation set:\n",
      "  Crack: 254\n",
      "  ExposedRebars: 104\n",
      "  Spalling: 477\n",
      "  Rust: 465\n",
      "  ACrack: 42\n",
      "  Rockpocket: 52\n",
      "  Hollowareas: 155\n",
      "  Efflorescence: 206\n",
      "  Cavity: 167\n",
      "  Wetspot: 144\n",
      "  Weathering: 392\n",
      "  Restformwork: 133\n",
      "  Graffiti: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "ann_dir_test = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/annotations/test\"\n",
    "\n",
    "# Get all annotation paths\n",
    "ann_paths_test = [os.path.join(ann_dir_test, sp) for sp in os.listdir(ann_dir_test)]\n",
    "label_counts_test = count_labels(ann_paths_test, LABELS)\n",
    "print(\"Label counts in validation set:\")\n",
    "for label, count in label_counts_test.items():\n",
    "    print(f\"  {label}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99934ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crack: 0.1471\n",
      "ExposedRebars: 0.1345\n",
      "Spalling: 0.1462\n",
      "Rust: 0.1348\n",
      "ACrack: 0.1265\n",
      "Rockpocket: 0.1672\n",
      "Hollowareas: 0.1413\n",
      "Efflorescence: 0.1353\n",
      "Cavity: 0.1406\n",
      "Wetspot: 0.1492\n",
      "Weathering: 0.1450\n",
      "Restformwork: 0.1865\n",
      "Graffiti: 0.1836\n"
     ]
    }
   ],
   "source": [
    "# Compute ratios of validation to training counts\n",
    "ratios = {\n",
    "    label: label_counts_test[label] / max(1, label_counts_train[label])\n",
    "    for label in LABELS\n",
    "}\n",
    "\n",
    "# Show ratios. Split will be done so that validation set has similar distribution.\n",
    "for label, ratio in ratios.items():\n",
    "    print(f\"{label}: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95c12f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting labels: 100%|██████████| 1040/1040 [00:00<00:00, 23116.71it/s]\n",
      "Counting labels: 100%|██████████| 5895/5895 [00:00<00:00, 24325.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ratios for validation to new training set:\n",
      "  Crack: 0.1788\n",
      "  ExposedRebars: 0.1856\n",
      "  Spalling: 0.1763\n",
      "  Rust: 0.1739\n",
      "  ACrack: 0.2029\n",
      "  Rockpocket: 0.1604\n",
      "  Hollowareas: 0.1645\n",
      "  Efflorescence: 0.1724\n",
      "  Cavity: 0.1797\n",
      "  Wetspot: 0.1812\n",
      "  Weathering: 0.1839\n",
      "  Restformwork: 0.1593\n",
      "  Graffiti: 0.1691\n",
      "\n",
      "Final label counts in new training set:\n",
      "  Crack: 1465\n",
      "  ExposedRebars: 652\n",
      "  Spalling: 2773\n",
      "  Rust: 2939\n",
      "  ACrack: 276\n",
      "  Rockpocket: 268\n",
      "  Hollowareas: 942\n",
      "  Efflorescence: 1299\n",
      "  Cavity: 1007\n",
      "  Wetspot: 817\n",
      "  Weathering: 2284\n",
      "  Restformwork: 615\n",
      "  Graffiti: 680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform splitting\n",
    "MIN = 0.13 # min ratio of val to train samples for each class\n",
    "r = 0.15 # split ratio\n",
    "n_val = int(len(ann_paths_train) * r)\n",
    "\n",
    "# Random Split with similar distribution\n",
    "success = False\n",
    "while not success:\n",
    "    # Split randomly\n",
    "    ann_paths_val = random.sample(ann_paths_train, n_val)\n",
    "    ann_paths_new_train = [ap for ap in ann_paths_train if ap not in ann_paths_val]\n",
    "\n",
    "    # Count labels\n",
    "    label_counts_val = count_labels(ann_paths_val, LABELS)\n",
    "    label_counts_new_train = count_labels(ann_paths_new_train, LABELS)\n",
    "\n",
    "    # Compute ratios\n",
    "    ratios = {\n",
    "    label: label_counts_val[label] / max(1, label_counts_new_train[label])\n",
    "    for label in LABELS\n",
    "    }\n",
    "\n",
    "    # Check if all ratios are above MIN\n",
    "    success = all(ratio >= MIN for ratio in ratios.values())\n",
    "\n",
    "    if success:\n",
    "        print (\"\\nRatios for validation to new training set:\")\n",
    "        for label, ratio in ratios.items():\n",
    "            print(f\"  {label}: {ratio:.4f}\")\n",
    "\n",
    "print(\"\\nFinal label counts in new training set:\")\n",
    "for label, count in label_counts_new_train.items():\n",
    "    print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4d1ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split images, annotaitions, and masks into new train/val sets\n",
    "train_samples = [os.path.splitext(os.path.basename(p))[0] for p in ann_paths_new_train]\n",
    "val_samples = [os.path.splitext(os.path.basename(p))[0] for p in ann_paths_val]\n",
    "\n",
    "# Create validation directories\n",
    "ann_dir_val = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/annotations/validation\"\n",
    "mask_dir_val = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/masks/validation\"\n",
    "img_dir_val = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data/images/validation\"\n",
    "for dir in [ann_dir_val, mask_dir_val, img_dir_val]:\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "# Train directories\n",
    "ann_dir_train = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/annotations/train\"\n",
    "mask_dir_train = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/masks/train\"\n",
    "img_dir_train = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data/images/train\"\n",
    "\n",
    "# Move validation samples to validation directories\n",
    "for sample in val_samples:\n",
    "\n",
    "    # Annotations\n",
    "    src_ann = os.path.join(ann_dir_train, f\"{sample}.json\")\n",
    "    dst_ann = os.path.join(ann_dir_val, f\"{sample}.json\")\n",
    "    if os.path.exists(src_ann):\n",
    "        shutil.move(src_ann, dst_ann)\n",
    "\n",
    "    # Masks\n",
    "    src_mask = os.path.join(mask_dir_train, f\"{sample}.png\")\n",
    "    dst_mask = os.path.join(mask_dir_val, f\"{sample}.png\")\n",
    "    if os.path.exists(src_mask):\n",
    "        shutil.move(src_mask, dst_mask)\n",
    "\n",
    "    # Images\n",
    "    src_img = os.path.join(img_dir_train, f\"{sample}.jpg\")\n",
    "    dst_img = os.path.join(img_dir_val, f\"{sample}.jpg\")\n",
    "    if os.path.exists(src_img):\n",
    "        shutil.move(src_img, dst_img)\n",
    "\n",
    "# Integrity checks\n",
    "ann_paths_train = [os.path.join(ann_dir_train, sp) for sp in os.listdir(ann_dir_train)]\n",
    "mask_paths_train = [os.path.join(mask_dir_train, sp) for sp in os.listdir(mask_dir_train)]\n",
    "img_paths_train = [os.path.join(img_dir_train, sp) for sp in os.listdir(img_dir_train)]\n",
    "\n",
    "ann_paths_val = [os.path.join(ann_dir_val, sp) for sp in os.listdir(ann_dir_val)]\n",
    "mask_paths_val = [os.path.join(mask_dir_val, sp) for sp in os.listdir(mask_dir_val)]\n",
    "img_paths_val = [os.path.join(img_dir_val, sp) for sp in os.listdir(img_dir_val)]\n",
    "\n",
    "# Check lengths\n",
    "assert len(ann_paths_train) == len(mask_paths_train) == len(img_paths_train)\n",
    "\n",
    "# Check that all samples in train set are correct\n",
    "assert all(os.path.splitext(os.path.basename(p))[0] in train_samples for p in ann_paths_train)\n",
    "assert all(os.path.splitext(os.path.basename(p))[0] in train_samples for p in mask_paths_train)\n",
    "assert all(os.path.splitext(os.path.basename(p))[0] in train_samples for p in img_paths_train)\n",
    "\n",
    "# Check that all samples in validation set are correct\n",
    "assert all(os.path.splitext(os.path.basename(p))[0] in val_samples for p in ann_paths_val)\n",
    "assert all(os.path.splitext(os.path.basename(p))[0] in val_samples for p in mask_paths_val)\n",
    "assert all(os.path.splitext(os.path.basename(p))[0] in val_samples for p in img_paths_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d2bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust names for consistency\n",
    "def adjust_filename(path, old_str, new_str):\n",
    "    folder = os.path.dirname(path)\n",
    "    old_name = os.path.basename(path)\n",
    "    new_name = old_name.replace(old_str, new_str)\n",
    "    new_path = os.path.join(folder, new_name)\n",
    "    os.rename(path, new_path)\n",
    "\n",
    "# Validation\n",
    "for ann_path, mask_path, img_path in zip(ann_paths_val, mask_paths_val, img_paths_val):\n",
    "    adjust_filename(ann_path, \"train\", \"validation\")\n",
    "    adjust_filename(mask_path, \"train\", \"validation\")\n",
    "    adjust_filename(img_path, \"train\", \"validation\")\n",
    "\n",
    "# Test\n",
    "ann_dir_test = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/annotations/test\"\n",
    "mask_dir_test = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data_clean/masks/test\"\n",
    "img_dir_test = \"/Users/diegovelasco/Desktop/Diego/FING/DLFCV-FinalProject/data/images/test\"\n",
    "\n",
    "ann_paths_test = [os.path.join(ann_dir_test, sp) for sp in os.listdir(ann_dir_test)]\n",
    "mask_paths_test = [os.path.join(mask_dir_test, sp) for sp in os.listdir(mask_dir_test)]\n",
    "img_paths_test = [os.path.join(img_dir_test, sp) for sp in os.listdir(img_dir_test)]\n",
    "\n",
    "for ann_path, mask_path, img_path in zip(ann_paths_test, mask_paths_test, img_paths_test):\n",
    "    adjust_filename(ann_path, \"validation\", \"test\")\n",
    "    adjust_filename(mask_path, \"validation\", \"test\")\n",
    "    adjust_filename(img_path, \"validation\", \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
